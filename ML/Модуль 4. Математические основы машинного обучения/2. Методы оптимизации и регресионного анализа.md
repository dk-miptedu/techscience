**Цели недели 2.**
    
    В результате обучения на этой неделе вы: 
    
    - узнаете, в каких задачах машинного обучения используются линейные модели
    - сможете формально поставить задачу линейной регрессии
    - познакомитесь с теоремой Гаусса-Маркова
    - научитесь использовать L1- и L2-регуляризации для решения задач машинного обучения
    - узнаете, что такое градиент функции
    - познакомитесь с методами оптимизации
    - научитесь решать задачи оптимизации градиентными методами
    
##  2.1 Производная и ее применения
    
Ссылка на видео: [https://disk.yandex.ru/i/4QWvlS_zGySqjg](https://disk.yandex.ru/i/4QWvlS_zGySqjg)  
    
##  2.2 Градиентная оптимизация    
Ссылка на видео: [https://disk.yandex.ru/i/M7OfUuQQ1tTL9w](https://disk.yandex.ru/i/M7OfUuQQ1tTL9w)  
    
##  2.3 Условная оптимизация

Ссылка на видео: [https://disk.yandex.ru/i/gXO5VsXXec3NXg](https://disk.yandex.ru/i/gXO5VsXXec3NXg)  
    
##  2.4 Решение задачи оптимизации градиентными методами

Ссылка на видео: [https://disk.yandex.ru/i/I0Nm1X39T9bLrA](https://disk.yandex.ru/i/I0Nm1X39T9bLrA)  
    
##  2.5 Линейные модели машинного обучения

Ссылка на видео: [https://disk.yandex.ru/i/Lrsx25nFNK0tkQ](https://disk.yandex.ru/i/Lrsx25nFNK0tkQ)  
    
##  2.6 Линейная регрессия

Ссылка на видео: [https://disk.yandex.ru/i/QHDMKRRX7LPFrQ](https://disk.yandex.ru/i/QHDMKRRX7LPFrQ)  
    
##  2.7 Теорема Гаусса-Маркова

Ссылка на видео: [https://disk.yandex.ru/i/dOgCGApl8HUcjg](https://disk.yandex.ru/i/dOgCGApl8HUcjg)  
    
##  2.8 L1 и L2 регуляризация

Ссылка на видео: [https://disk.yandex.ru/i/2ihEXA4hvmbdvQ](https://disk.yandex.ru/i/2ihEXA4hvmbdvQ)  
    
##  2.9 Решение линейной регрессии и анализ устойчивости решения

Ссылка на видео: [https://disk.yandex.ru/i/TBGr_udMvHlZig](https://disk.yandex.ru/i/TBGr_udMvHlZig)  
    
# **Дополнительные материалы**
    
##  2.10 Конспект занятия

![2.10.Конспект%20(Модуль%204%20Неделя%202).pdf](./assets/2.10.Конспект%20(Модуль%204%20Неделя%202).pdf)
    
## **Проверочные задания**

```python
import numpy as np

class LossAndDerivatives:
    @staticmethod
    def mse(X, Y, w):
        return np.mean((X.dot(w) - Y)**2)

    @staticmethod
    def mae(X, Y, w):
        return np.mean(np.abs(X.dot(w) - Y))

    @staticmethod
    def l2_reg(w):
        return np.sum(w**2)

    @staticmethod
    def l1_reg(w):
        return np.sum(np.abs(w))

    @staticmethod
    def no_reg(w):
        return 0.

    @staticmethod
    def l2_reg_derivative(w):
        return 2 * w

    @staticmethod
    def l1_reg_derivative(w):
        return np.sign(w)

    @staticmethod
    def no_reg_derivative(w):
        return np.zeros_like(w)
```

```python
import numpy as np

def mse_derivative(X, Y, w):
    n_observations = X.shape[0]
    # Расчет производной MSE
    gradient = (2/n_observations) * X.T.dot(X.dot(w) - Y)
    return gradient

```

```python
import numpy as np

@staticmethod
def mae_derivative(X, Y, w):
    predictions = X.dot(w)  # Вычисляем предсказания модели
    error = predictions - Y  # Вычисляем ошибку предсказания
    sign_error = np.sign(error)  # Получаем знак ошибки
    
    # Подготавливаем массив для градиентов с той же формой, что и w
    gradients = np.zeros_like(w)
    
    if Y.ndim == 1 or Y.shape[1] == 1:
        # Одномерный случай
        gradients = X.T.dot(sign_error) / X.shape[0]
    else:
        # Многомерный случай, обрабатываем каждую целевую переменную отдельно
        for i in range(Y.shape[1]):
            # Вычисляем градиент для каждой целевой переменной отдельно
            gradients[:, i] = (X.T.dot(sign_error[:, i]) / X.shape[0]).flatten()
    
    return gradients

```
