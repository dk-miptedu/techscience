
# 5. Метод опорных векторов. Оценка качества классификации. Методы кросс-валидации

- **Цели недели 5.**

В результате обучения на этой неделе вы: 

- познакомитесь с методом опорных векторов, используемым для задач классификации и регрессионного анализа
- узнаете способ создания нелинейного классификатора с помощью так называемого ядерного трюка (kernel trick)
- продолжите изучение метрик оценки качества классификации, разберете ROC-AUC
- научитесь использовать метод кросс-валидации для оценки качества модели

## 5.1 Метод опорных векторов

Ссылка на видео: [https://disk.yandex.ru/i/TDzNZLCQ1vLt2w](https://disk.yandex.ru/i/TDzNZLCQ1vLt2w)  

## 5.2 Нелинейный метод опорных векторов

Ссылка на видео: [https://disk.yandex.ru/i/iDoojs7GaoikRw](https://disk.yandex.ru/i/iDoojs7GaoikRw)  

## 5.3 Оценка качества классификации]
Ссылка на видео: [https://disk.yandex.ru/i/JF8eSPoS7JNoYw](https://disk.yandex.ru/i/JF8eSPoS7JNoYw)  

## 5.4 Методы кросс-валидации

Ссылка на видео: [https://disk.yandex.ru/i/xDmMVzAU1UDiTA](https://disk.yandex.ru/i/xDmMVzAU1UDiTA)  

## 5.5 Cross-validation riddle

Ссылка на видео: [https://disk.yandex.ru/i/A8qCw4ngUI56pA](https://disk.yandex.ru/i/A8qCw4ngUI56pA)  

**Дополнительные материалы**

## 5.6 Конспект занятия

![Конспект занятия](5.6.Конспект%20(Модуль%204%20Неделя%205).pdf)

**Проверочные задания**

## 5.7 Итоговый тест 2

![Итоговый тест 2](./assets/5.7%20Итоговый%20тест%202.pdf)

## 5.8 Задание на программирование 5

```python
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.metrics import accuracy_score

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim


def rbf(x_1, x_2, sigma=1.):
    ### YOUR CODE HERE
    x_1 = x_1.unsqueeze(1)  # Добавляем измерение для x_1: (n_samples, 1, n_features)
    x_2 = x_2.unsqueeze(0)  # Добавляем измерение для x_2: (1, m_samples, n_features)
    sq_diff = torch.pow(x_1 - x_2, 2).sum(2)
    kernel_matrix = torch.exp(-sq_diff / (2 * sigma ** 2))
    
    return kernel_matrix

def hinge_loss(scores, labels):
     assert len(scores.shape) == 1
    assert len(labels.shape) == 1
    return torch.mean(torch.clamp(1 - scores * labels, min=0)) ### YOUR CODE HERE


class SVM(BaseEstimator, ClassifierMixin):
    @staticmethod
    def linear(x_1, x_2):
         return x_1 @ x_2.T ### YOUR CODE HERE
    
    def __init__(
        self,
        lr: float=1e-3,
        epochs: int=2,
        batch_size: int=64,
        lmbd: float=1e-4,
        kernel_function=None,
        verbose: bool=False,
    ):
        self.lr = lr
        self.epochs = epochs
        self.batch_size = batch_size
        self.lmbd = lmbd
        self.kernel_function = kernel_function or SVM.linear
        self.verbose = verbose
        self.fitted = False

    def __repr__(self):
        return 'SVM model, fitted: {self.fitted}'

    def fit(self, X, Y):
        assert (np.abs(Y) == 1).all()
        n_obj = len(X)
        X, Y = torch.FloatTensor(X), torch.FloatTensor(Y)
        K = self.kernel_function(X, X).float()

        self.betas = torch.full((n_obj, 1), fill_value=0.001, dtype=X.dtype, requires_grad=True)
        self.bias = torch.zeros(1, requires_grad=True) 
        
        optimizer = optim.SGD((self.betas, self.bias), lr=self.lr)
        for epoch in range(self.epochs):
            perm = torch.randperm(n_obj) 
            sum_loss = 0.  
            for i in range(0, n_obj, self.batch_size):
                batch_inds = perm[i:i + self.batch_size]
                x_batch = X[batch_inds] 
                y_batch = Y[batch_inds]
                k_batch = K[batch_inds]
                
                optimizer.zero_grad()     
                ### YOUR CODE HERE
                preds = (k_batch @ self.betas).flatten() + self.bias 
                ###
                preds = preds.flatten()
                loss = self.lmbd * self.betas[batch_inds].T @ k_batch @ self.betas + hinge_loss(preds, y_batch)
                loss.backward()   
                optimizer.step()  
                sum_loss += loss.item() 

            if self.verbose: print("Epoch " + str(epoch) + ", Loss: " + str(sum_loss / self.batch_size))

        self.X = X
        self.fitted = True
        return self

    def predict_scores(self, batch):
        with torch.no_grad():
            batch = torch.from_numpy(batch).float()
            K = self.kernel_function(batch, self.X)
            ### YOUR CODE HERE
            return (K @ self.betas + self.bias).flatten() 

    def predict(self, batch):
        scores = self.predict_scores(batch)
        answers = np.full(len(batch), -1, dtype=np.int64)
        answers[scores > 0] = 1
        return answers
```

### Дополнительное задание на программирование "ML Pipeline"

[IPython](./assets/Lab1_part2_ml_pipeline.ipynb)





